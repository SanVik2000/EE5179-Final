{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XkjWkWAhE_-"
      },
      "source": [
        "# Tutorial 8: Deep Autoencoders "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7MO-7IShFAA"
      },
      "source": [
        "In this tutorial, we will take a closer look at autoencoders (AE). \n",
        "\n",
        "The main purpose of an autoencoder is to encode an input signal such as an image (using an encoder) into a smaller feature vector, which is inturn, then used to reconstruct the signal back (using a decoder). The feature vector is called the \"bottleneck\" of the network as we aim to compress the input data into a smaller amount of features. This property is useful in many applications, in particular in compressing data or comparing images on a metric beyond pixel-level comparisons. For the decoder, we will make use of 'deconvolutions' or 'transposed' convolutions for scaling up feature maps in height and width. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcugSWXmhFAB"
      },
      "outputs": [],
      "source": [
        "## Standard libraries\n",
        "import os\n",
        "import json\n",
        "import math\n",
        "import numpy as np \n",
        "\n",
        "## Imports for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline \n",
        "from IPython.display import set_matplotlib_formats\n",
        "set_matplotlib_formats('svg', 'pdf') # For export\n",
        "from matplotlib.colors import to_rgb\n",
        "import matplotlib\n",
        "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
        "import seaborn as sns\n",
        "sns.reset_orig()\n",
        "sns.set()\n",
        "\n",
        "## Progress bar\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "## PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "# Torchvision\n",
        "import torchvision\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision import transforms\n",
        "\n",
        "import torch.multiprocessing\n",
        "torch.multiprocessing.set_sharing_strategy('file_system')\n",
        "\n",
        "# PyTorch Lightning\n",
        "try:\n",
        "    import pytorch_lightning as pl\n",
        "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
        "    !pip install --quiet pytorch-lightning>=1.4\n",
        "    import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
        "\n",
        "# Tensorboard extension (for visualization purposes later)\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "%load_ext tensorboard\n",
        "\n",
        "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
        "DATASET_PATH = \"data\"\n",
        "# Path to the folder where the pretrained models are saved\n",
        "CHECKPOINT_PATH = \"tutorial8\"\n",
        "\n",
        "# Setting the seed\n",
        "pl.seed_everything(42)\n",
        "\n",
        "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"Device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XC6Wm27rhFAH"
      },
      "source": [
        "In this tutorial, we work with the CIFAR10 dataset. CIFAR10 dataset contains 50,000 training and 10,000 validation images with each image having 3 color channels and 32x32 pixels large. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhO_Br-zhFAH"
      },
      "outputs": [],
      "source": [
        "# Transformations applied on each image => only make them a tensor\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,),(0.5,))])\n",
        "\n",
        "# Loading the training dataset. We need to split it into a training and validation part\n",
        "train_dataset = CIFAR10(root='data', train=True, transform=transform, download=True)\n",
        "pl.seed_everything(42)\n",
        "train_set, val_set = torch.utils.data.random_split(train_dataset, [45000, 5000])\n",
        "\n",
        "# Loading the test set\n",
        "test_set = CIFAR10(root='data', train=False, transform=transform, download=True)\n",
        "\n",
        "# We define a set of data loaders that we can use for various purposes later.\n",
        "train_loader = data.DataLoader(train_set, batch_size=256, shuffle=True, drop_last=True, pin_memory=True, num_workers=4)\n",
        "val_loader = data.DataLoader(val_set, batch_size=256, shuffle=False, drop_last=False, num_workers=4)\n",
        "test_loader = data.DataLoader(test_set, batch_size=256, shuffle=False, drop_last=False, num_workers=4)\n",
        "\n",
        "def get_train_images(num):\n",
        "    return torch.stack([train_dataset[i][0] for i in range(num)], dim=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFn7zmiWhFAI"
      },
      "source": [
        "## Building the autoencoder\n",
        "\n",
        "In general, an autoencoder consists of an **encoder** that maps the input $x$ to a lower-dimensional feature vector $z$, and a **decoder** that reconstructs the input $\\hat{x}$ from $z$. We train the model by comparing $x$ to $\\hat{x}$ and optimizing the parameters to increase the similarity between $x$ and $\\hat{x}$. See below for a small illustration of the autoencoder framework."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPHqzQ9OhFAJ"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/SanVik2000/EE5179/main/Tutorial-7/autoencoder_visualization.svg\" style=\"width:350px;height:200px;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCgmh1-3hFAJ"
      },
      "source": [
        "We first start by implementing the encoder. The encoder effectively consists of a deep convolutional network, where we scale down the image layer-by-layer using strided convolutions. After downscaling the image three times, we flatten the features and apply linear layers. The latent representation $z$ is therefore a vector of size *d* which can be flexibly selected. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvWS0bl4QRRI"
      },
      "source": [
        "# Encoder\n",
        "\n",
        "Conv2d($hid$,3,1,2) -> Act Fn -> Conv2d($hid$,3,1, 2) -> Act Fn -> Conv2d($2*hid$,3,1) -> Act Fn -> Conv2d($2*hid$,3,1,2) -> Act Fn -> Linear(latent_dim)\n",
        "\n",
        "Legends:\n",
        "Conv2d - Convolutional2dlayer\n",
        "Conv2d(x,y,z,w) - x = out_chanels | y = kernel_size | z = padding | w = stride\n",
        "Act Fn - Activation function that is passed as asrguments to the"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azNM8cbxhFAK"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    \n",
        "    def __init__(self, \n",
        "                 num_input_channels : int, \n",
        "                 base_channel_size : int, \n",
        "                 latent_dim : int, \n",
        "                 act_fn : object = nn.GELU):\n",
        "        \"\"\"\n",
        "        Inputs: \n",
        "            - num_input_channels : Number of input channels of the image. For CIFAR, this parameter is 3\n",
        "            - base_channel_size : Number of channels we use in the first convolutional layers. Deeper layers might use a duplicate of it.\n",
        "            - latent_dim : Dimensionality of latent representation z\n",
        "            - act_fn : Activation function used throughout the encoder network\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        c_hid = base_channel_size\n",
        "        self.net = nn.Sequential(# Fill your model code here)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXZkSI7FhFAL"
      },
      "source": [
        "We obtain the decoder by just taking the mirror image/ flipping the encoder. The only difference is that the vanilla convolutional layers are replaced with transposed convolutions to upscale the features. Transposed convolutions can be imagined as adding the stride to the input instead of the output, and can thus upscale the input. For an illustration of a `nn.ConvTranspose2d` layer with kernel size 3, stride 2, and padding 1, see below (figure credit - [Vincent Dumoulin and Francesco Visin](https://arxiv.org/abs/1603.07285)):\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://raw.githubusercontent.com/SanVik2000/EE5179/main/Tutorial-7/deconvolution.gif\" width=\"250px\"></center> \n",
        "\n",
        "Overall, the decoder can be implemented as follows:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtDYtfrZR70B"
      },
      "source": [
        "# Decoder\n",
        "\n",
        "Linear($2*hid$) -> Act. Fn. -> ConvTrans($2*hid$, 3, 1, 1, 2) -> Act. Fn. -> ConvTrans($2*hid$, 3, 1) -> Act. Fn. -> ConvTrans($hid$, 3, 1, 1, 2) -> Act. Fn. -> ConvTrans(3, 3, 1, 1, 2) -> Tanh()\n",
        "\n",
        "Legends:\n",
        "\n",
        "ConvTrans - ConvTranspose2d layer\n",
        "ConvTrans(x, y, z, w, a) - x = out_channels | y = kernel_size | z = padding | w = output_padding | a = stride"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4wSzMJAhFAL"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    \n",
        "    def __init__(self, \n",
        "                 num_input_channels : int, \n",
        "                 base_channel_size : int, \n",
        "                 latent_dim : int, \n",
        "                 act_fn : object = nn.GELU):\n",
        "        \"\"\"\n",
        "        Inputs: \n",
        "            - num_input_channels : Number of channels of the image to reconstruct. For CIFAR, this parameter is 3\n",
        "            - base_channel_size : Number of channels we use in the last convolutional layers. Early layers might use a duplicate of it.\n",
        "            - latent_dim : Dimensionality of latent representation z\n",
        "            - act_fn : Activation function used throughout the decoder network\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        c_hid = base_channel_size\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 2*16*c_hid),\n",
        "            act_fn()\n",
        "        )\n",
        "        self.net = nn.Sequential( # Fill your model code here )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        x = x.reshape(x.shape[0], -1, 4, 4)\n",
        "        x = self.net(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPyixU6bhFAM"
      },
      "source": [
        "The encoder and decoder networks we chose here are relatively simple. Usually, more complex networks are applied, especially when using a ResNet-based architecture. For example, see [VQ-VAE](https://arxiv.org/abs/1711.00937) and [NVAE](https://arxiv.org/abs/2007.03898) (although the papers discuss architectures for VAEs, they can equally be applied to standard autoencoders). \n",
        "\n",
        "In a final step, we add the encoder and decoder together into the autoencoder architecture. We define the autoencoder as PyTorch Lightning Module to simplify the needed training code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIx2XAlFhFAN"
      },
      "outputs": [],
      "source": [
        "class Autoencoder(pl.LightningModule):\n",
        "    \n",
        "    def __init__(self, \n",
        "                 base_channel_size: int, \n",
        "                 latent_dim: int, \n",
        "                 encoder_class : object = Encoder,\n",
        "                 decoder_class : object = Decoder,\n",
        "                 num_input_channels: int = 3, \n",
        "                 width: int = 32, \n",
        "                 height: int = 32):\n",
        "        super().__init__()\n",
        "        # Saving hyperparameters of autoencoder\n",
        "        self.save_hyperparameters() \n",
        "        # Creating encoder and decoder\n",
        "        self.encoder = # Fill your code here\n",
        "        self.decoder = # Fill your code here\n",
        "        # Example input array needed for visualizing the graph of the network\n",
        "        self.example_input_array = torch.zeros(2, num_input_channels, width, height)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        The forward function takes in an image and returns the reconstructed image\n",
        "        \"\"\"\n",
        "        z = # Fill your code here\n",
        "        x_hat = # Fill your code here\n",
        "        return x_hat\n",
        "    \n",
        "    def _get_reconstruction_loss(self, batch):\n",
        "        \"\"\"\n",
        "        Given a batch of images, this function returns the reconstruction loss (MSE in our case)\n",
        "        \"\"\"\n",
        "        x, _ = batch # We do not need the labels\n",
        "        x_hat = # Fill your code here\n",
        "        loss = # Fill your code here using torch.functional.MSE Loss\n",
        "        loss = loss.sum(dim=[1,2,3]).mean(dim=[0])\n",
        "        return loss\n",
        "    \n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
        "        # Using a scheduler is optional but can be helpful.\n",
        "        # The scheduler reduces the LR if the validation performance hasn't improved for the last N epochs\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
        "                                                         mode='min', \n",
        "                                                         factor=0.2, \n",
        "                                                         patience=20, \n",
        "                                                         min_lr=5e-5)\n",
        "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"}\n",
        "    \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss = self._get_reconstruction_loss(batch)                             \n",
        "        self.log('train_loss', loss)\n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        loss = self._get_reconstruction_loss(batch)\n",
        "        self.log('val_loss', loss)\n",
        "    \n",
        "    def test_step(self, batch, batch_idx):\n",
        "        loss = self._get_reconstruction_loss(batch)\n",
        "        self.log('test_loss', loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23bQXK89hFAO"
      },
      "source": [
        "For the loss function, we use the mean squared error (MSE). However, MSE has also some considerable disadvantages. Usually, MSE leads to blurry images where small noise/high-frequent patterns are removed as those cause a very low error.\n",
        "\n",
        "Additionally, comparing two images using MSE does not necessarily reflect their visual similarity. For instance, suppose the autoencoder reconstructs an image shifted by one pixel to the right and bottom. Although the images are almost identical, we can get a higher loss than predicting a constant pixel value for half of the image (see code below). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdXP6G_ohFAO"
      },
      "outputs": [],
      "source": [
        "def compare_imgs(img1, img2, title_prefix=\"\"):\n",
        "    # Calculate MSE loss between both images\n",
        "    loss = F.mse_loss(img1, img2, reduction=\"sum\")\n",
        "    # Plot images for visual comparison\n",
        "    grid = torchvision.utils.make_grid(torch.stack([img1, img2], dim=0), nrow=2, normalize=True, range=(-1,1))\n",
        "    grid = grid.permute(1, 2, 0)\n",
        "    plt.figure(figsize=(4,2))\n",
        "    plt.title(f\"{title_prefix} Loss: {loss.item():4.2f}\")\n",
        "    plt.imshow(grid)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "for i in range(2):\n",
        "    # Load example image\n",
        "    img, _ = train_dataset[i]\n",
        "    img_mean = img.mean(dim=[1,2], keepdims=True)\n",
        "\n",
        "    # Shift image by one pixel\n",
        "    SHIFT = 1\n",
        "    img_shifted = torch.roll(img, shifts=SHIFT, dims=1)\n",
        "    img_shifted = torch.roll(img_shifted, shifts=SHIFT, dims=2)\n",
        "    img_shifted[:,:1,:] = img_mean\n",
        "    img_shifted[:,:,:1] = img_mean\n",
        "    compare_imgs(img, img_shifted, \"Shifted -\")\n",
        "\n",
        "    # Set half of the image to zero\n",
        "    img_masked = img.clone()\n",
        "    img_masked[:,:img_masked.shape[1]//2,:] = img_mean\n",
        "    compare_imgs(img, img_masked, \"Masked -\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCQ7YZMzhFAO"
      },
      "source": [
        "### Training the model\n",
        "\n",
        "During the training, we want to keep track of the learning progress by seeing reconstructions made by our model. For this, we implement a callback object in PyTorch Lightning which will add reconstructions every $N$ epochs to our tensorboard:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkdis79FhFAP"
      },
      "outputs": [],
      "source": [
        "class GenerateCallback(pl.Callback):\n",
        "    \n",
        "    def __init__(self, input_imgs, every_n_epochs=1):\n",
        "        super().__init__()\n",
        "        self.input_imgs = input_imgs # Images to reconstruct during training\n",
        "        self.every_n_epochs = every_n_epochs # Only save those images every N epochs (otherwise tensorboard gets quite large)\n",
        "        \n",
        "    def on_epoch_end(self, trainer, pl_module):\n",
        "        if trainer.current_epoch % self.every_n_epochs == 0:\n",
        "            # Reconstruct images\n",
        "            input_imgs = self.input_imgs.to(pl_module.device)\n",
        "            with torch.no_grad():\n",
        "                pl_module.eval()\n",
        "                reconst_imgs = pl_module(input_imgs)\n",
        "                pl_module.train()\n",
        "            # Plot and add to tensorboard\n",
        "            imgs = torch.stack([input_imgs, reconst_imgs], dim=1).flatten(0,1)\n",
        "            grid = torchvision.utils.make_grid(imgs, nrow=2, normalize=True, range=(-1,1))\n",
        "            trainer.logger.experiment.add_image(\"Reconstructions\", grid, global_step=trainer.global_step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emfBQWo-hFAQ"
      },
      "outputs": [],
      "source": [
        "def train_cifar(latent_dim):\n",
        "    # Create a PyTorch Lightning trainer with the generation callback\n",
        "    trainer = pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH, f\"cifar10_{latent_dim}\"), \n",
        "                         gpus=1 if str(device).startswith(\"cuda\") else 0, \n",
        "                         max_epochs=250, \n",
        "                         callbacks=[ModelCheckpoint(save_weights_only=True),\n",
        "                                    GenerateCallback(get_train_images(8), every_n_epochs=10),\n",
        "                                    LearningRateMonitor(\"epoch\")])\n",
        "    trainer.logger._log_graph = True         # If True, we plot the computation graph in tensorboard\n",
        "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
        "    \n",
        "    # Check whether pretrained model exists. If yes, load it and skip training\n",
        "    pretrained_filename = os.path.join(CHECKPOINT_PATH, f\"cifar10_{latent_dim}.ckpt\")\n",
        "    if os.path.isfile(pretrained_filename):\n",
        "        print(\"Found pretrained model, loading...\")\n",
        "        model = Autoencoder.load_from_checkpoint(pretrained_filename)\n",
        "    else:\n",
        "        model = Autoencoder(base_channel_size=32, latent_dim=latent_dim)\n",
        "        trainer.fit(model, train_loader, val_loader)\n",
        "    # Test best model on validation and test set\n",
        "    val_result = trainer.test(model, val_loader, verbose=False)\n",
        "    test_result = trainer.test(model, test_loader, verbose=False)\n",
        "    result = {\"test\": test_result, \"val\": val_result}\n",
        "    return model, result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q084aUIRhFAQ"
      },
      "source": [
        "### Comparing latent dimensionality\n",
        "\n",
        "When training an autoencoder, we need to choose a dimensionality for the latent representation $z$. The higher the latent dimensionality, the better we expect the reconstruction to be. However, the idea of autoencoders is to *compress* data. Hence, we are also interested in keeping the dimensionality low. To find the best tradeoff, we can train multiple models with different latent dimensionalities. The original input has $32\\times 32\\times 3 = 3072$ pixels. Keeping this in mind, a reasonable choice for the latent dimensionality might be between 64 and 384:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0ABhoc2hFAR"
      },
      "outputs": [],
      "source": [
        "model_dict = {}\n",
        "for latent_dim in [64, 128, 256, 384]:\n",
        "    model_ld, result_ld = train_cifar(latent_dim)\n",
        "    model_dict[latent_dim] = {\"model\": model_ld, \"result\": result_ld}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1dVb8_QhFAR"
      },
      "source": [
        "After training the models, we can plot the reconstruction loss over the latent dimensionality to get an intuition how these two properties are correlated:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnFnSbTwhFAR"
      },
      "outputs": [],
      "source": [
        "latent_dims = sorted([k for k in model_dict])\n",
        "val_scores = [model_dict[k][\"result\"][\"val\"][0][\"test_loss\"] for k in latent_dims]\n",
        "\n",
        "fig = plt.figure(figsize=(6,4))\n",
        "plt.plot(latent_dims, val_scores, '--', color=\"#000\", marker=\"*\", markeredgecolor=\"#000\", markerfacecolor=\"y\", markersize=16)\n",
        "plt.xscale(\"log\")\n",
        "plt.xticks(latent_dims, labels=latent_dims)\n",
        "plt.title(\"Reconstruction error over latent dimensionality\", fontsize=14)\n",
        "plt.xlabel(\"Latent dimensionality\")\n",
        "plt.ylabel(\"Reconstruction error\")\n",
        "plt.minorticks_off()\n",
        "plt.ylim(0,100)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQbRqIQkhFAS"
      },
      "source": [
        "As we initially expected, the reconstruction loss goes down with increasing latent dimensionality. For our model and setup, the two properties seem to be exponentially (or double exponentially) correlated. To understand what these differences in reconstruction error mean, we can visualize example reconstructions of the four models: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZD54V743hFAS"
      },
      "outputs": [],
      "source": [
        "def visualize_reconstructions(model, input_imgs):\n",
        "    # Reconstruct images\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        reconst_imgs = model(input_imgs.to(model.device))\n",
        "    reconst_imgs = reconst_imgs.cpu()\n",
        "    \n",
        "    # Plotting\n",
        "    imgs = torch.stack([input_imgs, reconst_imgs], dim=1).flatten(0,1)\n",
        "    grid = torchvision.utils.make_grid(imgs, nrow=4, normalize=True, range=(-1,1))\n",
        "    grid = grid.permute(1, 2, 0)\n",
        "    plt.figure(figsize=(7,4.5))\n",
        "    plt.title(f\"Reconstructed from {model.hparams.latent_dim} latents\")\n",
        "    plt.imshow(grid)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJkgHaXGhFAT"
      },
      "outputs": [],
      "source": [
        "input_imgs = get_train_images(4)\n",
        "for latent_dim in model_dict:\n",
        "    visualize_reconstructions(model_dict[latent_dim][\"model\"], input_imgs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dW5s-98VhFAT"
      },
      "source": [
        "Clearly, the smallest latent dimensionality can only save information about the rough shape and color of the object, but the reconstructed image is extremely blurry and it is hard to recognize the original object in the reconstruction. With 128 features, we can recognize some shapes again although the picture remains blurry. The models with the highest two dimensionalities reconstruct the images quite well. The difference between 256 and 384 is marginal at first sight but can be noticed when comparing, for instance, the backgrounds of the first image (the 384 features model more of the pattern than 256)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSzaaQ94hFAT"
      },
      "source": [
        "### Out-of-distribution images\n",
        "\n",
        "Before continuing with the applications of autoencoder, we can actually explore some limitations of our autoencoder. For example, what happens if we try to reconstruct an image that is clearly out of the distribution of our dataset? We expect the decoder to have learned some common patterns in the dataset, and thus might in particular fail to reconstruct images that do not follow these patterns.\n",
        "\n",
        "The first experiment we can try is to reconstruct noise. We, therefore, create two images whose pixels are randomly sampled from a uniform distribution over pixel values, and visualize the reconstruction of the model (feel free to test different latent dimensionalities):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxDc60mKhFAU"
      },
      "outputs": [],
      "source": [
        "rand_imgs = torch.rand(2, 3, 32, 32) * 2 - 1\n",
        "visualize_reconstructions(model_dict[384][\"model\"], rand_imgs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAvPoZ66hFAU"
      },
      "source": [
        "The reconstruction of the noise is quite poor, and seems to introduce some rough patterns. As the input does not follow the patterns of the CIFAR dataset, the model has issues reconstructing it accurately.\n",
        "\n",
        "We can also check how well the model can reconstruct other manually-coded patterns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZERAWWm8hFAU"
      },
      "outputs": [],
      "source": [
        "plain_imgs = torch.zeros(4, 3, 32, 32)\n",
        "\n",
        "# Single color channel\n",
        "plain_imgs[1,0] = 1 \n",
        "# Checkboard pattern\n",
        "plain_imgs[2,:,:16,:16] = 1\n",
        "plain_imgs[2,:,16:,16:] = -1\n",
        "# Color progression\n",
        "xx, yy = torch.meshgrid(torch.linspace(-1,1,32), torch.linspace(-1,1,32))\n",
        "plain_imgs[3,0,:,:] = xx\n",
        "plain_imgs[3,1,:,:] = yy\n",
        "\n",
        "visualize_reconstructions(model_dict[384][\"model\"], plain_imgs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIwfDiW4hFAV"
      },
      "source": [
        "The plain, constant images are reconstructed relatively good although the single color channel contains some noticeable noise. The hard borders of the checkboard pattern are not as sharp as intended, as well as the color progression, both because such patterns never occur in the real-world pictures of CIFAR.\n",
        "\n",
        "In general, autoencoders tend to fail reconstructing high-frequent noise (i.e. sudden, big changes across few pixels) due to the choice of MSE as loss function (see our previous discussion about loss functions in autoencoders). Small misalignments in the decoder can lead to huge losses so that the model settles for the expected value/mean in these regions. For low-frequent noise, a misalignment of a few pixels does not result in a big difference to the original image. However, the larger the latent dimensionality becomes, the more of this high-frequent noise can be accurately reconstructed. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4t40ValLhFAV"
      },
      "source": [
        "### Generating new images\n",
        "\n",
        "Variational autoencoders are a generative version of the autoencoders because we regularize the latent space to follow a Gaussian distribution. However, in vanilla autoencoders, we do not have any restrictions on the latent vector. So what happens if we would actually input a randomly sampled latent vector into the decoder? Let's find it out below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORR6YTmjhFAV"
      },
      "outputs": [],
      "source": [
        "model = model_dict[384][\"model\"]\n",
        "latent_vectors = torch.randn(8, model.hparams.latent_dim, device=model.device)\n",
        "with torch.no_grad():\n",
        "    imgs = model.decoder(latent_vectors)\n",
        "    imgs = imgs.cpu()\n",
        "\n",
        "grid = torchvision.utils.make_grid(imgs, nrow=4, normalize=True, range=(-1,1), pad_value=0.5)\n",
        "grid = grid.permute(1, 2, 0)\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.imshow(grid)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyOMRAkGhFAW"
      },
      "source": [
        "As we can see, the generated images more look like art than realistic images. As the autoencoder was allowed to structure the latent space in whichever way it suits the reconstruction best, there is no incentive to map every possible latent vector to realistic images. Furthermore, the distribution in latent space is unknown to us and doesn't necessarily follow a multivariate normal distribution. Thus, we can conclude that vanilla autoencoders are indeed not generative."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAVY3gtehFAW"
      },
      "source": [
        "## Finding visually similar images\n",
        "\n",
        "One application of autoencoders is to build an image-based search engine to retrieve visually similar images. This can be done by representing all images as their latent dimensionality, and find the closest $K$ images in this domain. The first step to such a search engine is to encode all images into $z$. In the following, we will use the training set as a search corpus, and the test set as queries to the system.\n",
        "\n",
        "<span style=\"color: #880000\">(Warning: the following cells can be computationally heavy for a weak CPU-only system.</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laKBNHVrhFAX"
      },
      "outputs": [],
      "source": [
        "# We use the following model throughout this section. \n",
        "# If you want to try a different latent dimensionality, change it here!\n",
        "model = model_dict[384][\"model\"] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHn5pcbshFAX"
      },
      "outputs": [],
      "source": [
        "def embed_imgs(model, data_loader):\n",
        "    # Encode all images in the data_laoder using model, and return both images and encodings\n",
        "    img_list, embed_list = [], []\n",
        "    model.eval()\n",
        "    for imgs, _ in tqdm(data_loader, desc=\"Encoding images\", leave=False):\n",
        "        with torch.no_grad():\n",
        "            z = model.encoder(imgs.to(model.device))\n",
        "        img_list.append(imgs)\n",
        "        embed_list.append(z)\n",
        "    return (torch.cat(img_list, dim=0), torch.cat(embed_list, dim=0))\n",
        "\n",
        "train_img_embeds = embed_imgs(model, train_loader)\n",
        "test_img_embeds = embed_imgs(model, test_loader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4AuVLSKhFAX"
      },
      "source": [
        "After encoding all images, we just need to write a function that finds the closest $K$ images and returns (or plots) those:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKGPFYvTXcFj"
      },
      "outputs": [],
      "source": [
        "def find_similar_images(query_img, query_z, key_embeds, K=8):\n",
        "    # Find closest K images. We use the euclidean distance here but other like cosine distance can also be used.\n",
        "    dist = torch.cdist(query_z[None,:], key_embeds[1], p=2)\n",
        "    dist = dist.squeeze(dim=0)\n",
        "    dist, indices = torch.sort(dist)\n",
        "    # Plot K closest images\n",
        "    imgs_to_display = torch.cat([query_img[None], key_embeds[0][indices[:K]]], dim=0)\n",
        "    grid = torchvision.utils.make_grid(imgs_to_display, nrow=K+1, normalize=True, range=(-1,1))\n",
        "    grid = grid.permute(1, 2, 0)\n",
        "    plt.figure(figsize=(12,3))\n",
        "    plt.imshow(grid)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsQGs7YLYzhf"
      },
      "outputs": [],
      "source": [
        "# Plot the closest images for the first N test images as example\n",
        "for i in range(8):\n",
        "    find_similar_images(test_img_embeds[0][i], test_img_embeds[1][i], key_embeds=train_img_embeds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhJlsTPShFAY"
      },
      "source": [
        "Based on our autoencoder, we see that we are able to retrieve many similar images to the test input. In particular, in row 4, we can spot that some test images might not be that different from the training set as we thought (same poster, just different scaling/color scaling). We also see that although we haven't given the model any labels, it can cluster different classes in different parts of the latent space (airplane + ship, animals, etc.). This is why autoencoders can also be used as a pre-training strategy for deep networks, especially when we have a large set of unlabeled images (often the case). However, it should be noted that the background still plays a big role in autoencoders while it doesn't for classification. Hence, we don't get \"perfect\" clusters and need to finetune such models for classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JapJq6ihFAZ"
      },
      "source": [
        "### Tensorboard clustering\n",
        "\n",
        "Another way of exploring the similarity of images in the latent space is by dimensionality-reduction methods like PCA or T-SNE. Luckily, Tensorboard provides a nice interface for this and we can make use of it in the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5TB3q5uY2eS"
      },
      "outputs": [],
      "source": [
        "# We use the following model throughout this section. \n",
        "# If you want to try a different latent dimensionality, change it here!\n",
        "model = model_dict[384][\"model\"]\n",
        "\n",
        "# Create a summary writer\n",
        "writer = SummaryWriter(\"tensorboard/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b72BWk46hFAa"
      },
      "source": [
        "The function `add_embedding` allows us to add high-dimensional feature vectors to TensorBoard on which we can perform clustering. What we have to provide in the function are the feature vectors, additional metadata such as the labels, and the original images so that we can identify a specific image in the clustering. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lv1Ipk0KY841"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorboard as tb\n",
        "tf.io.gfile = tb.compat.tensorflow_stub.io.gfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-Pl7L6PhFAa"
      },
      "outputs": [],
      "source": [
        "# Note: the embedding projector in tensorboard is computationally heavy.\n",
        "# Reduce the image amount below if your computer struggles with visualizing all 10k points\n",
        "NUM_IMGS = len(test_set) \n",
        "\n",
        "writer.add_embedding(test_img_embeds[1][:NUM_IMGS], # Encodings per image\n",
        "                     metadata=[test_set[i][1] for i in range(NUM_IMGS)], # Adding the labels per image to the plot\n",
        "                     label_img=(test_img_embeds[0][:NUM_IMGS]+1)/2.0) # Adding the original images to the plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYjN9j1ZhFAb"
      },
      "source": [
        "Finally, we can run tensorboard to explore similarities among images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTgE27Y6ZANy"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir tensorboard/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEI_DTbzZEQ0"
      },
      "outputs": [],
      "source": [
        "# Closing the summary writer\n",
        "writer.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vX6ouOGhFAc"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In this tutorial, we have implemented our own autoencoder on small RGB images and explored various properties of the model. \n",
        "\n",
        "Repeat the process for latent dimensions of 128 and 256 and compare the properties observed in the above applications."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}